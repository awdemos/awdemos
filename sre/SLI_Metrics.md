Automated data collection from systems is crucial for feeding **Service Level Indicators (SLIs)** and ensuring **Service Level Objectives (SLOs)** are met. **Site Reliability Engineering (SRE)** as a professional field of work suggests monitoring a core set of an applications metrics known as the **four golden signals**:

1. **Latency**
2. **Traffic**
3. **Errors**
4. **Saturation**

These metrics provide a comprehensive overview of system health and are easily measurable, even if they don't directly correspond to your SLOs. Developing systems to gather, store, and display these metrics can run concurrently with tailoring your own SLOs and SLIs.

- **Latency** refers to the time required for a task to complete or a request to receive a response. It's important to specify if you're measuring all responses or only successful ones. For websites, latency indicates the speed at which users receive feedback.

- **Traffic** measures the workload your system manages, such as the volume of a message queue or the rate of HTTP requests. It reflects the demand placed on your system.

- **Errors** represent failed or incorrect responses. For web services, this could involve analyzing server logs for HTTP status codes to gauge failure rates. Measuring correctness may require custom metrics or specific testing.

- **Saturation** gauges system capacity usage, covering metrics like memory, CPU, and network load. It indicates how close your system is to its maximum capacity.

These signals offer a foundational perspective on system health. Monitoring them for any dependent services further enhances your insight. However, the true measure of system performance comes from your specific SLIs, which trigger alerts when SLOs are at risk. Crafting these SLIs often demands significant engineering work, with the complexity and utility of the data influencing the effort required.

Consider web application latency as an example. An SLO might state that the homepage should load in under half a second for 95% of requests. Various methods exist for measuring server response times, but these don't account for delays introduced by load balancers, CDNs, or internet transit.

Synthetic metrics, generated by external systems simulating user requests, can provide a broader view, including network delays. However, they don't capture browser rendering times, which can be significant. Integrating monitoring into your JavaScript to track both response and rendering times may require additional engineering or third-party services like Google Analytics.

Choosing SLI implementations involves balancing data quality against collection costs. Starting with server log latency is practical, with plans to expand into browser-level monitoring as needed.

Modern monitoring solutions typically adopt a pull model, collecting metrics from application components and storing them in a time series database. This approach, supported by standards like those from the Prometheus open-source project, allows for detailed temporal analysis.

Prometheus, for example, organizes metrics with labels, facilitating granular analysis without overwhelming data points. It also supports percentile calculations, making it easier to analyze response times or job durations over time.

Monitoring systems drive both alerts and dashboards. Dashboards visualize SLIs and the four golden signals, providing immediate insight into system health and potential issues. Alerts, on the other hand, notify teams of urgent problems requiring immediate attention or less critical issues that can be addressed in the near term.
