{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Midterm Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Defining your Problem and Audience",
        "",
        "✅ Deliverables",
        "",
        "#### Problem Statement ",
        "",
        "Create an intelligent Bible study assistant that utilizes LLMs to enhance contextual understanding of scripture. This tool will enable users to pose questions, and the AI will provide answers grounded in the Bible, by accurately identifying and synthesizing information from relevant verses, chapters, and cross-references, promoting deeper comprehension and reducing misinterpretations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Propose a Solution",
        "",
        "✅ Deliverables",
        "",
        "#### Application Framework",
        "",
        "<img src=\\'img/arch.png\\' />",
        "",
        "- LLM ",
        "    - \\'gpt-4o\\' is used for agent reasoning. ",
        "- Embedding Model",
        "    - my fine tuned embedding model based on Snowflake/snowflake-arctic-embed-l",
        "- Orchestration",
        "    - There are two nodes in the graph: 1 agent node, 1 action node. The agent node does some reasoning based on the user queries and determines which tool in the action node to use. The action node provides a tool set for the agent node to use. In the first phase of this project, a RAG chain is the only available tool. The book of Genesis is saved in the vector database in RAG.",
        "- Vector Database",
        "    - Qdrant as it is a popular choirc for RAG. It is open sourced, excels at performing fast, accurate, and scalable vector similarity searches.",
        "- Monitoring",
        "- Evaluation",
        "    - RAGAS is used to generate synthetic data and evaluate the agent langgraph by using the following metrics",
        "        - Context Recall",
        "        - Faithfulness",
        "        - Factual correctness",
        "        - answer relevancy",
        "        - context entity recall",
        "        - noise sensitivity relevant",
        "- User Interface",
        "    - Chainlit as it is open sourced and ease to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Dealing with the Data",
        "",
        "✅ Deliverables",
        "1. Describe all of your data sources and external APIs, and describe what you’ll use them for.",
        "",
        "    - The book of Genesis is downloaded to local from https://www.vatican.va/archive/bible/genesis/documents/bible_genesis_en.html by using curl command. And then it is loaded by using langchain director loader.",
        "",
        "2. Describe the default chunking strategy that you will use.  Why did you make this decision?",
        "",
        "    - RecursiveCharacterTextSplitter is used to chunk the document. This text splitter is designed to split text recursively based on specified characters. It prioritizes splitting by certain characters (like paragraphs or sentences) before resorting to splitting by character length. The parameter chunk_overlap = 100 is useful for maintaining context and avoiding loss of information when processing the chunks independently.",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Building a Quick End-to-End Prototype",
        "",
        "✅ Deliverables",
        "",
        "https://huggingface.co/spaces/kcheng0816/BibleStudy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU cohere langchain_cohere langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "ba50f775-3957-4d88-9a88-43acc6966dda"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "### Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os",
        "from dotenv import load_dotenv",
        "",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4",
        "",
        "os.environ[\\'LANGCHAIN_TRACING_V2\\'] = \\'true\\'",
        "os.environ[\\'LANGCHAIN_PROJECT\\'] = f\\'AIE5 - Midterm Sample - {uuid4().hex[0:8]}\\'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current",
            "                                 Dload  Upload   Total   Spent    Left  Speed",
            "100  216k  100  216k    0     0   117k      0  0:00:01  0:00:01 --:--:--  118k"
          ]
        }
      ],
      "source": [
        "!curl https://www.vatican.va/archive/bible/genesis/documents/bible_genesis_en.html -o data/bible_genesis_en.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.",
            ""
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader",
        "",
        "path = \\'data/\\'",
        "loader = DirectoryLoader(path, glob=\\'*.html\\')",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "307"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter",
        "",
        "text_splitter = RecursiveCharacterTextSplitter(",
        "    chunk_size = 750,",
        "    chunk_overlap = 100",
        ")",
        "",
        "split_documents = text_splitter.split_documents(docs)",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Model and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings",
        "",
        "base_embedding_model = HuggingFaceEmbeddings(model_name=\\'Snowflake/snowflake-arctic-embed-l\\')",
        "",
        "#from langchain_openai.embeddings import OpenAIEmbeddings",
        "",
        "#embedding_model = OpenAIEmbeddings(model=\\'text-embedding-3-small\\')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore",
        "from qdrant_client import QdrantClient",
        "from qdrant_client.http.models import Distance, VectorParams",
        "",
        "client = QdrantClient(\\':memory:\\')",
        "",
        "client.create_collection(",
        "    collection_name=\\'rust_docs\\',",
        "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),",
        ")",
        "",
        "vector_store = QdrantVectorStore(",
        "    client=client,",
        "    collection_name=\\'rust_docs\\',",
        "    embedding=base_embedding_model,",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = vector_store.add_documents(documents=split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\\'k\\': 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve(state):",
        "  retrieved_docs = retriever.invoke(state[\\'question\\']]",
        "  return {\\'context\\' : retrieved_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate",
        "",
        "RAG_PROMPT = \\'\\'\\",
        "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.",
        "",
        "### Question",
        "{question}",
        "",
        "### Context",
        "{context}",
        "\\'\\'\\",
        "",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI",
        "from langchain.chat_models import init_chat_model",
        "from langchain_core.rate_limiters import InMemoryRateLimiter",
        "",
        "rate_limiter = InMemoryRateLimiter(",
        "    requests_per_second=1,  # <-- make a request once every 1 seconds!!",
        "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,",
        "    max_bucket_size=10,  # Controls the maximum burst size.",
        ")",
        "",
        "llm = init_chat_model(\\'gpt-4o-mini\\', rate_limiter=rate_limiter)",
        "",
        "#hit langsmith rate limit",
        "#llm = ChatOpenAI(model=\\'gpt-4o-mini\\')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(state):",
        "  docs_content = \\'\\\\n\\\\n\\'.join(doc.page_content for doc in state[\\'context\\']]",
        "  messages = rag_prompt.format_messages(question=state[\\'question\\'], context=docs_content)",
        "  response = llm.invoke(messages)",
        "  return {\\'response\\' : response.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph",
        "from typing_extensions import List, TypedDict",
        "from langchain_core.documents import Document",
        "",
        "class State(TypedDict):",
        "  question: str",
        "  context: List[Document]",
        "  response: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])",
        "graph_builder.add_edge(START, \\'retrieve\\')",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = graph.invoke({\\'question\\' : \\'Why was Rust created?\\'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "How to run tests?"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\\'response\\']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool Belt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "tool_belt = [",
        "    ai_rag_tool",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### LangGraph Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binding the tools to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "llm = init_chat_model(\\'gpt-4o\\', temperature=0, rate_limiter=rate_limiter)",
        "llm_with_tools = llm.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END",
        "from langchain_core.messages import AnyMessage",
        "from langgraph.graph.message import add_messages",
        "from typing import TypedDict, Annotated",
        "from langchain_core.documents import Document",
        "",
        "",
        "class AgentState(TypedDict):",
        "    messages: Annotated[list[AnyMessage], add_messages]",
        "    context:List[Document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\\n\\n",
        "def call_mode(state):\\n",
        "    messages = state[\\'messages\\'];\\n",
        "    response = llm_with_tools.invoke(messages);\\n",
        "    return {\\n",
        "        \\'messages\\': [response],\\n",
        "        \\'context\\': state.get(\\'context\\', [])\\n",
        "    }\\n",
        "\\n",
        "tool_node = ToolNode(tool_belt)"
    ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def should_continue(state):",
        "    last_message = state[\\'messages\\'][-1]",
        "",
        "    if last_message.tool_calls:",
        "        return \\'action\\'",
        "    ",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the graph"
      ]
    }
